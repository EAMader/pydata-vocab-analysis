{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## Visualizing\n",
    "\n",
    "An important part of exploring a data set is producing easy to understand visualizations that describe or summarize the data. With structured data we have many well known visualizations like scatter plots, pie charts, bar charts, etc. How can we use some of these with our vocabulary analysis? Are there any ways we can characterize our data that are especially suited for text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from vocab_analysis import *\n",
    "\n",
    "import answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df = pd.read_pickle('./data/tokenized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we have segments to our data. Ultimately, we will be trying to create models to predict membership in these segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments = [\n",
    "    pd.Series(jobs_df['experience'] == '5+', name='5+ years experience'),\n",
    "    pd.Series(jobs_df['experience'] == '2-5', name='2-5 years experience'),\n",
    "    pd.Series(jobs_df['experience'] == '1-2', name='1-2 years experience'),\n",
    "    pd.Series(jobs_df['education'] == 'ms-or-phd-needed', name='Master\\'s Degree or PhD'),\n",
    "    pd.Series(jobs_df['education'] == 'bs-degree-needed', name='Bachelor\\'s Degree'),\n",
    "    pd.Series(jobs_df['education'] == 'associate-needed', name='Associate\\'s Degree'),\n",
    "    pd.Series(jobs_df['is_hourly'], name='Hourly'),\n",
    "    pd.Series(jobs_df['is_part_time'], name='Part-time'),\n",
    "    pd.Series(jobs_df['is_supervisor'], name='Supervising'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pickle these for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/segments.pickle', 'wb') as fp:\n",
    "    pickle.dump(segments, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the distribution of $\\sum_{d \\in D}{\\mbox{TF}(t, d)}$ vs $\\mbox{IDF}(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_tfidf_df = calculate_avg_tfidf(jobs_df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can visualize the data is to look at the distribution of $\\mbox{TF}$ vs $\\mbox{IDF}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tfidf_freqs(avg_tfidf_df, n, title='sum TF vs IDF', ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "    ax.scatter(avg_tfidf_df.sum_tf, avg_tfidf_df.idf, c=avg_tfidf_df.avg_tfidf, cmap=plt.cm.coolwarm)\n",
    "    ax.set_xbound(0, avg_tfidf_df.sum_tf.max())\n",
    "    ax.set_ybound(0, avg_tfidf_df.idf.max())\n",
    "    ax.set_xlabel('sum TF')\n",
    "    ax.set_ylabel('IDF')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf_freqs(avg_tfidf_df, len(jobs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tfidf_df.query('sum_tf > 20000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tfidf_df.query('sum_tf > 10000 and idf > 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our formulation of $\\mbox{TF.IDF}$ has a limitation: some of our vocabulary have such high $\\mbox{TF}$ that the $\\mbox{IDF}$ does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds\n",
    "\n",
    "Word clouds are an easy to digest visualization that is especially suited for text.\n",
    "\n",
    "To produce a wordcloud you need your vocabulary and an associated weight. Sometimes this is just the occurrences in a document or corpus ($\\mbox{n_t}$ from our $\\mbox{TF.IDF}$ formula). We will be using average $\\mbox{TF.IDF}$, but let's also look at the wordclouds for $\\mbox{TF}$ and $\\mbox{IDF}$ for the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud(avg_tfidf_df['avg_tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(avg_tfidf_df['sum_tf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(avg_tfidf_df['idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also look at the wordclouds by segment. In order to make these visualization easier to use, a helper function has been included in `vocab_analysis.py` which calculates $\\mbox{TF.IDF}$ and produces the scatter plot of $\\mbox{TF}$ vs $\\mbox{IDF}$, and $\\mbox{TF.IDF}$ based wordclouds for the overall corpus as well as each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyze(jobs_df, 'tokens', segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our attention to improving our processing. We need to do something to deal with how messy our tokens are.\n",
    "\n",
    "### NEXT => [4. Stemming and Lemmatization](4. Stemming and Lemmatization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
