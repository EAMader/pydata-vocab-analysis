{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## n-Grams and Sentence Boundary Detection\n",
    "\n",
    "n-Grams are fixed width sequence of words pulled from a text. Let's use the following sentence as an example.\n",
    "**Note**: when n is less then 5, they are sometimes given a special name\n",
    "- 1-gram = unigram\n",
    "- 2-gram = bigram\n",
    "- 3-gram = trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 'the quick brown fox jumped over the lazy dog'.split(' ')\n",
    "\n",
    "n = 1\n",
    "print('unigrams')\n",
    "print([tokens[i:i+n] for i in xrange(len(tokens) - n + 1)])\n",
    "\n",
    "n = 2\n",
    "print('bigrams')\n",
    "print([tokens[i:i+n] for i in xrange(len(tokens) - n + 1)])\n",
    "\n",
    "n = 3\n",
    "print('trigrams')\n",
    "print([tokens[i:i+n] for i in xrange(len(tokens) - n + 1)])\n",
    "\n",
    "n = 4\n",
    "print('4-grams')\n",
    "print([tokens[i:i+n] for i in xrange(len(tokens) - n + 1)])\n",
    "\n",
    "n = 5\n",
    "print('5-grams')\n",
    "print([tokens[i:i+n] for i in xrange(len(tokens) - n + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from vocab_analysis import *\n",
    "\n",
    "import answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df = pd.read_pickle('./data/cleaned.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/segments.pickle') as fp:\n",
    "    segments = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem though. What if our sequences run across a sentence boundary? Although these ngrams would likely be rare, for low n this can still cause problems. We will need to split our documents into sentences.\n",
    "\n",
    "NLTK comes function for splitting text into sentences - `PunktSentenceTokenizer`.\n",
    "\n",
    "(Sentence boundary disambiguation [wikipedia](https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation))  \n",
    "(PunktTokenizer [docs](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer))  \n",
    "(Punkt algorithm [paper](https://www.linguistics.ruhr-uni-bochum.de/~kiss/publications/compling2005_KS27.01final.pdf))\n",
    "\n",
    "The idea in text segmentation like this is to either find the boundaries, are find the segments. Punkt finds the boundaries by a combination of heuristics and collocation learning for identifying abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_tokenize import tokenize\n",
    "from my_lemmatize import lemmatize, english_lemmas\n",
    "from my_stopword_removal import stopword_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def lemma_sentences(job_description):\n",
    "    \"\"\"\n",
    "    This function takes a job description and splits it into sentences\n",
    "    Parameters\n",
    "    ----------\n",
    "    job_description : str\n",
    "        The text of the job description\n",
    "    Returns\n",
    "    ----------\n",
    "    list[str]\n",
    "        the list of sentences\n",
    "    \"\"\"\n",
    "    sentences = sent_detector.tokenize(job_description)\n",
    "    return [stopword_removal(lemmatize(tokenize(sentence), english_lemmas)) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df['sentences'] = jobs_df['description'].apply(lemma_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/segments.pickle') as fp:\n",
    "    segments = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we sentences, we can generate ngrams.\n",
    "\n",
    "It worth considering the performance for choose large n.\n",
    "\n",
    "Although the increase in terms per document is not prohibitive, when this drastically increase the cost of calculating $\\mbox{TF.IDF}$ since the terms will be much more sparse. Our data set is small enough that we can got to 5-grams, but for very large corpora trigrams are probably a safer limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_func(n):\n",
    "    \"\"\"\n",
    "    This function creates an ngram extracting function with an appropriate name.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        the n of the ngrams to be produced\n",
    "    Returns\n",
    "    ----------\n",
    "    Callable[str] -> list[str]\n",
    "        the ngram generating function\n",
    "    \"\"\"\n",
    "    assert n>1, 'n must be greater than 1'\n",
    "    def fun(sentences):\n",
    "        return ['-'.join(ngram) for sentence in sentences for ngram in nltk.ngrams(sentence, n)]\n",
    "    if n == 2:\n",
    "        fun.func_name = 'bigrams'\n",
    "    elif n == 3:\n",
    "        fun.func_name = 'trigrams'\n",
    "    else:\n",
    "        fun.func_name = 'n_{}_grams'.format(n)\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df['bigrams'] = jobs_df['sentences'].apply(ngram_func(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze(jobs_df, 'bigrams', segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are even more meaningful than just the lemmas. However, TF vs IDF plot has become useless. Also, note that our vocabulary has gone from 18155 lemmas to 306245 bigrams, a 1687% increase.\n",
    "\n",
    "Observations validating some intuitions\n",
    "- \"year-experience\" appears to be more important the more experience is required\n",
    "- \"associate-degree\", \"bachelor-degree\", \"master-degree\" are important for their respective education levels\n",
    "\n",
    "There are some oddities\n",
    "- \"silver-bullet\" is a prominent bigram for jobs requiring a graduate degree\n",
    "- \"engineer-ui\" and \"ui-engineer\" are important for jobs requiring 5+ years experience\n",
    "\n",
    "Let's look at \"silver-bullet\" oddity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_avg_tfidf_df = calculate_avg_tfidf(jobs_df['bigrams'])\n",
    "bigram_index, bigram_inv_index = build_indexes(jobs_df['bigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\n",
    "    \"silver bullet\", \n",
    "    jobs_df['description'], \n",
    "    bigram_index, \n",
    "    bigram_inv_index, \n",
    "    bigram_avg_tfidf_df['idf'],\n",
    "    lambda q: ngram_func(2)([stopword_removal(lemmatize(tokenize(q), english_lemmas))])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that \"Silver Bullet\" is a specific company with multiple jobs in our data. This is the danger with n-grams. Because they are more meaningful to our data for larger n, the conclusions are also less generalizable for larger n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jobs_df['trigrams'] = jobs_df['sentences'].apply(ngram_func(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(jobs_df, 'trigrams', segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 491719 trigrams, a 61% increase from bigrams, and a 2708% increase from lemmas.\n",
    "\n",
    "Observations\n",
    "- There are some formulaic phrases in our data leading to \"equal-opportun-employ\" being prominent in all segments\n",
    "- \"high-school-diploma\", \"hours-per-week\", and \"valid-driver-license\" are prominent for hourly jobs\n",
    "\n",
    "Oddities\n",
    "- \"colorado-spring-co\" is prominent for some segments, this certainly not generalizable \n",
    "- \"engineer-ui-engineer\" and \"ui-engineer-ui\" are prominent for jobs requireing 5+ years of experience\n",
    "\n",
    "Let's look into the \"engineer-ui-engineer\" oddity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_avg_tfidf_df = calculate_avg_tfidf(jobs_df['trigrams'])\n",
    "trigram_index, trigram_inv_index = build_indexes(jobs_df['trigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search(\n",
    "    \"engineer ui engineer\", \n",
    "    jobs_df['description'], \n",
    "    trigram_index, \n",
    "    trigram_inv_index, \n",
    "    trigram_avg_tfidf_df['idf'],\n",
    "    lambda q: ngram_func(3)([stopword_removal(lemmatize(tokenize(q), english_lemmas))])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the bottom of the description, you will see a classic search engine optimization (SEO) tactic - repeating key words to boost your $\\mbox{TF}$ for some keywords.\n",
    "\n",
    "Although this is an oddity, it is valuable information. If one were attempting classify jobs by industry, how might these repeats affect the modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df['quadrigrams'] = jobs_df['sentences'].apply(ngram_func(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze(jobs_df, 'quadrigrams', segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 497154 4-grams, a 1% increase from trigrams.\n",
    "\n",
    "Observations\n",
    "- formulaic phrases have almost completely taken over\n",
    "- social work appears to be prominent type of job in our data\n",
    "\n",
    "Oddities\n",
    "- \"sale-sale-sale-sale\" appears, is likely another instance of SEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quadrigram_avg_tfidf_df = calculate_avg_tfidf(jobs_df['quadrigrams'])\n",
    "quadrigram_index, quadrigram_inv_index = build_indexes(jobs_df['quadrigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search(\n",
    "    \"sale sale sale sale\", \n",
    "    jobs_df['description'], \n",
    "    trigram_index, \n",
    "    trigram_inv_index, \n",
    "    trigram_avg_tfidf_df['idf'],\n",
    "    lambda q: ngram_func(3)([stopword_removal(lemmatize(tokenize(q), english_lemmas))])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What have we learned about our data set?\n",
    "\n",
    "- We should be able to distinguish between our segments using lemmas with stop words removed, and bigrams of the lemmas.\n",
    "- Some jobs are using SEO to boost their $\\mbox{TF}$\n",
    "- Sales, social work, medical work are common types in our data\n",
    "- Colorado appears to be overrepresented in our data\n",
    "- There are certain lemmas that appear common across all segments\n",
    "  - \"manage\" - \"manage\", \"manager\"\n",
    "  - \"experience\" - \"experience\", \"experienced\"\n",
    "  - \"sale\" - \"sale\", \"sales\"\n",
    "  - \"service\" - \"service\", \"services\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_fun(lemma_sentences, imports=['nltk'], star_imports=['my_tokenize', 'my_lemmatize', 'my_stopword_removal'], \n",
    "         sent_detector=sent_detector)\n",
    "save_fun(ngram_func, imports=['nltk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df.to_pickle('./data/ngrams.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take what we've learned and try and apply it to model-building\n",
    "\n",
    "### NEXT => [7. Modeling](7. Modeling.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
