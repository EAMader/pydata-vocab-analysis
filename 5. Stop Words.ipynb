{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## Stop words\n",
    "\n",
    "There is a term used in document search for words like \"and\", \"to\", and \"the\" - _stop words_. They are generally words that have a high enough average $\\mbox{TF}$ that their low $\\mbox{IDF}$ does not balance them out.\n",
    "\n",
    "NLTK provides us with a list of English words often considered stop words. Removing stop words is not a technique that should always be used, and what is and is not a stop word is dependent on the task at hand.\n",
    "\n",
    "Let's take a look at NLTK's stop words.\n",
    "\n",
    "(Stop words [wikipedia](https://en.wikipedia.org/wiki/Stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from vocab_analysis import *\n",
    "\n",
    "import answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df = pd.read_pickle('./data/lemmatized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/segments.pickle') as fp:\n",
    "    segments = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that they include tokens like \"shouldn\" and \"t\" which only exist due to tokenization.\n",
    "\n",
    "### Exercise 3: finding new stop words\n",
    "\n",
    "Let's try and find new words to add to our set of stop words. Keep in mind what the meaning of these words are. Although \"manag\" and \"experi\" occur very often, they are still meaningfull. You are looking for words that have high average $\\mbox{TF.IDF}$, but seem to lack important meaning.\n",
    "\n",
    "First we will look at $\\mbox{TF.IDF}$ values for NLTK's stop words, and then we will look for new candidates.\n",
    "\n",
    "Feel free to do this however you like - deciding a threshold for one of our values or manual selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma_avg_tfidf_df = calculate_avg_tfidf(jobs_df['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemma_avg_tfidf_df.sort_values('avg_tfidf', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_avg_tfidf_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_avg_tfidf_df[lemma_avg_tfidf_df.index.to_series().apply(lambda s: s in stopwords)].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lemma_avg_tfidf_df` dataframe contains the candidates for new stop words. Explore it by sorting by the different values, and look at different strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_lemma_avg_tfidf = lemma_avg_tfidf_df[lemma_avg_tfidf_df.index.to_series().apply(lambda s: s not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_lemma_avg_tfidf.sort_values('avg_tfidf', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_lemma_avg_tfidf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_stopwords = None\n",
    "\n",
    "if new_stopwords is None:\n",
    "    raise NotImplementedError(\"Find additions to the list of stop words\")\n",
    "    \n",
    "# new_stopwords = answers.additional_stopwords # uncomment this, and comment the above lines to skip this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_stopwords = stopwords | new_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stopword_removal(terms):\n",
    "    \"\"\"\n",
    "    This function removes stop words from a list of terms\n",
    "    Parameters\n",
    "    ----------\n",
    "    terms : list[str]\n",
    "        a list of terms from which to remove stop words\n",
    "    Returns\n",
    "    ----------\n",
    "    list[str]\n",
    "        a list of terms with the stop words removed\n",
    "    \"\"\"\n",
    "    return [s for s in terms if s not in custom_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df['cleaned_lemmas'] = jobs_df['lemmas'].apply(stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the effects of stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze(jobs_df, 'cleaned_lemmas', segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_fun(stopword_removal, custom_stopwords=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df.to_pickle('./data/cleaned.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Again, if you picked stop words that are very different from those in the answers module this analysis may not be applicable.\n",
    "\n",
    "These terms seems much more meaningful, but we still have a few terms appearing to be dominant - \"management\", \"experience\", \"sale\", \"service\". These terms are important to the overall context.\n",
    "\n",
    "1. n-grams: These should be more distinct for different segments since the set of sequences of terms is much larger than the set of terms. The large the value of n the more distinct we can expect complimentary segments to be. The downside is that not all ngrams are meaningful, but these meaningless ngrams are generally not common.\n",
    "\n",
    "We will be using n-grams.\n",
    "\n",
    "### NEXT => [6. n-Grams](6. n-Grams.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
