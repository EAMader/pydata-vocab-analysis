{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The first thing we will do is split the job description text into tokens.  \n",
    "A token is an element (word, character, symbol) of a string. For example, the string \"Cows eat grass\" can be tokenized into words or characters:\n",
    "- \"Cows\", \"eat\", \"grass\"\n",
    "- 'C', 'o', 'w', 's', ' ', 'e', 'a', 't', ' ', 'g', 'r', 'a', 's', 's'\n",
    "\n",
    "(Tokenization [wikipedia](https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis))\n",
    "\n",
    "Tokenization is a text segmentation process that reduces a string into a list of the tokens (elements) that make up the string. In natural language processing (NLP) it is generally used to refer the process of taking text and producing a list of words. Tokens are also used to refer to the characters of a string.\n",
    "\n",
    "Tokenization is often the first process applied to a piece of text, and because of this it heavily affect downstream processes. This can be complicated because there isn't necessarily a \"correct\" way to tokenize. Depending on language, kind of document, and downstream processes tokenization can be vastly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from vocab_analysis import *\n",
    "\n",
    "import answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df = pd.read_csv('data/job_descriptions.tsv', sep='\\t', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "\n",
    "Tokenization in English is often implemented with heuristics or regular expressions. Regular expressions can be used in one of two ways: to identify boundaries, or to identify tokens. Here are some common regular expressions for tokenizing English.\n",
    "\n",
    "Here are some questions to consider when doing tokenization.\n",
    "- How do I want to treat punctuation?\n",
    "  - Should \"word.\" tokenize to [\"word\", \".\"], [\"word.\"], or [\"word\"])?\n",
    "- How do I want to treat contractions (e.g. should we)?\n",
    "  - Should we pre-treat contractions by expanding all instances in the text?\n",
    "    - \"won't\" to \"will not\" then tokenize to [\"will\", \"not\"]\n",
    "  - Should we keep contractions together? Or break them up?\n",
    "    - \"won't\" tokenizes to [\"won't\"] or [\"won\", \"'\", \"t\"]\n",
    "  - Are we dealing with a more formal version English, and contractions are rare?\n",
    "- How do we want to treat numbers?\n",
    "  - Should \"A1\" tokenize to [\"A1\"] or [\"A\", \"1\"]?\n",
    "- How do we treat hyphens?\n",
    "  - Should \"State-of-the-art\" tokenize to [\"State-of-the-art\"], [\"State\", \"of\", \"the\", \"art\"], or [\"State\", \"-\", \"of\", \"-\", \"the\", \"-\", \"art\"]\n",
    "  \n",
    "Implement your tokenizer. When using regular expression based tokenizers, the common approaches are to either use the regular expression to identify tokens and extract all matches, or identify gaps and split the string. Use the example below to test out your tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_job_description = \"\"\"\n",
    "This is a description for a generic job.\n",
    "\n",
    "The employee is expected have the following:\n",
    "    1. an A1 certifcation (recent or renewed)\n",
    "    2. experience in widget-widget interaction\n",
    "    \n",
    "She/he will be expected to be stand for 3-4 hours at a time.\n",
    "She/he won't be expected to actually create widgets.\n",
    "\n",
    "Full-time\n",
    "Salary : $50,000/yr\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(job_description):\n",
    "    \"\"\"\n",
    "    This function takes a job description and returns a list of tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "    job_description : str\n",
    "        The text of the job description\n",
    "    Returns\n",
    "    ----------\n",
    "    list[str]\n",
    "        The list of tokens in the job description\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Implement the tokenizer')\n",
    "\n",
    "# tokenize = answers.tokenize # uncomment this, and comment the above function to skip this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenize(example_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are happy with your tokenization, let's tokenize the descriptions, save our work, and move on to analysis using $\\mbox{TF.IDF}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df['tokens'] = jobs_df['description'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df.to_pickle('data/tokenized.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also save our tokenization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_fun(tokenize, imports=['nltk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT => [2. TF.IDF](2. TF.IDF.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
