{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## Modeling\n",
    "\n",
    "Now that we have explored the vocabulary of the different segments of this corpus, let's see if we can predict whether or not a document belongs in a given segment. We can use what we've learned about the vocabulary to make educated guesses about what features we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import feature_extraction as skfeatex\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn import tree as sktree\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from vocab_analysis import *\n",
    "\n",
    "import answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './data/ngrams.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e90c38668132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjobs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/ngrams.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/althomas/anaconda/envs/vocabana/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/althomas/anaconda/envs/vocabana/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 return read_wrapper(\n\u001b[0;32m---> 92\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/althomas/anaconda/envs/vocabana/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     64\u001b[0m         f, fh = _get_handle(path, 'rb',\n\u001b[1;32m     65\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/althomas/anaconda/envs/vocabana/lib/python2.7/site-packages/pandas/io/common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# Python 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;31m# Python 3 and encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './data/ngrams.pickle'"
     ]
    }
   ],
   "source": [
    "jobs_df = pd.read_pickle('./data/ngrams.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `TfidfVectorizer` from scikit-learn to generate our features. It uses an analyzer to process text and then creates feature values based the $\\mbox{TF.IDF}$ of a term within a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skfeatex.text.TfidfVectorizer.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all of our functions back so we can use them as analyzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_tokenize import tokenize\n",
    "from my_lemmatize import lemmatize, english_lemmas\n",
    "from my_stopword_removal import stopword_removal\n",
    "from my_lemma_sentences import lemma_sentences\n",
    "from my_ngram_func import ngram_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `token_analyzer` will only tokenize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_analyzer(description):\n",
    "    return tokenize(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lemma_analyzer` will tokenize and lemmatize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemma_analyzer(description):\n",
    "    return lemmatize(tokenize(description), english_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_lemma_analyzer`, will tokenize, lemmatize, and then remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_lemma_analyzer(description):\n",
    "    return stopword_removal(lemmatize(tokenize(description), english_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bigram_analyzer` will split into sentences, tokenize, lemmatize, remove stop words, and then generate bigrams as our terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_analyzer(description):\n",
    "    lemmatized_sentences = lemma_sentences(description)\n",
    "    bigrams = ngram_func(2)(lemmatized_sentences)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trigram_analyzer` will split into sentences, tokenize, lemmatize, remove stop words, and then generate trigrams as our terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_analyzer(description):\n",
    "    lemmatized_sentences = lemma_sentences(description)\n",
    "    trigrams = ngram_func(3)(lemmatized_sentences)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `full_analyzer` will split into sentences, tokenize, lemmatize, remove stop words, and then generate bigrams and trigrams. It will use the cleaned lemmas, bigrams and trigrams as our terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_analyzer(description):\n",
    "    lemmatized_sentences = lemma_sentences(description)\n",
    "    unigrams = [unigram for sentence in lemmatized_sentences for unigram in sentence]\n",
    "    bigrams = ngram_func(2)(lemmatized_sentences)\n",
    "    trigrams = ngram_func(3)(lemmatized_sentences)\n",
    "    return unigrams + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tasks are\n",
    "- education: predict what level of education is needed for a job (none, associate-needed, bs-degree-needed, ms-or-phd-needed)\n",
    "- experience: predict how many years of experience is needed for a job (none, 1-2, 2-5, 5+)\n",
    "- is_hourly: predict whether a job is an hourly or not (True, False)\n",
    "- is_part_time: predict whether a job is part time or not (True, False)\n",
    "- is_supervisor: predict whether a job is a supervisory position or not (True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = ['education', 'experience', 'is_hourly', 'is_part_time', 'is_supervisor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covering how to use scikit-learn is outside the scope of this tutorial. If you want to know more about using scikit-learn, check out Sebastian Rashka's tutorial [_Learning scikit-learn -- An Introduction to Machine Learning in Python_](https://www.youtube.com/watch?v=9fOWryQq9J8). \n",
    "\n",
    "We'll be modeling with decision trees ([wiki]()) ([_Python for Data Science_ by Joe McCarthy](http://nbviewer.jupyter.org/github/gumption/Python_for_Data_Science/blob/master/Python_for_Data_Science_all.ipynb#4.-Using-Python-to-Build-and-Use-a-Simple-Decision-Tree-Classifier)). Rather than go into the details of decision trees, let's look at one.\n",
    "\n",
    "![tree](tree.png)\n",
    "\n",
    "This is a tree that was built for the is_hourly task.\n",
    "\n",
    "I've simplified working with these models by creating some widgets that let us modify some of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: finding the right features\n",
    "\n",
    "Let's try and find the right features for these tasks.\n",
    "\n",
    "I've pre-built all the models with my analyzers. Find the best performing set of features and parameters for each task.\n",
    "\n",
    "**Note**: If you want to try your own analyzers, uncomment the following cell to rename the current results folder and create your own. Know that each model can take 10 seconds to over a minute to build. It will also rename the saved features, which will also take a few minutes to regenerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! mv ./results/ ./results-pre-built/\n",
    "# ! mkdir ./results\n",
    "# ! mv ./data/all_features.pickle ./data/all_features_pre_built.pickle\n",
    "# ! mv ./data/all_featurizers.pickle ./data/all_featurizers_pre_built.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurization_approaches = OrderedDict()\n",
    "featurization_approaches['tokens'] = token_analyzer\n",
    "featurization_approaches['lemmas'] = lemma_analyzer\n",
    "featurization_approaches['clean_lemmas'] = clean_lemma_analyzer\n",
    "featurization_approaches['bigrams'] = bigram_analyzer\n",
    "featurization_approaches['trigrams'] = trigram_analyzer\n",
    "featurization_approaches['full'] = full_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = './data/all_features.pickle'\n",
    "featurizers_path = './data/all_featurizers.pickle'\n",
    "if os.path.exists(features_path):\n",
    "    print('Loading features')\n",
    "    with open(features_path) as fp:\n",
    "        all_features = pickle.load(fp)\n",
    "    print('Loading featurizers')\n",
    "    with open(featurizers_path) as fp:\n",
    "        all_featurizers = pickle.load(fp)\n",
    "else:\n",
    "    all_features = {}\n",
    "    all_featurizers = {}\n",
    "    for name, analyzer in featurization_approaches.items():\n",
    "        print(name)\n",
    "        featurizer = skfeatex.text.TfidfVectorizer(analyzer=analyzer)\n",
    "        features = featurizer.fit_transform(jobs_df['description'])\n",
    "        all_features[name] = features\n",
    "        all_featurizers[name] = featurizer.get_feature_names()\n",
    "    print('Saving features')\n",
    "    with open(features_path, 'wb') as out:\n",
    "        pickle.dump(all_features, out)\n",
    "    print('Saving featurizers')\n",
    "    with open(featurizers_path, 'wb') as out:\n",
    "        pickle.dump(all_featurizers, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(task, features, max_depth, min_samples_leaf):\n",
    "    save_path = './results/{}-{}-{}-{}.results.pickle'.format(\n",
    "        task, features, max_depth, min_samples_leaf\n",
    "    )\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path) as fp:\n",
    "            train_sizes, train_scores, test_scores, preds, model = pickle.load(fp)\n",
    "    else:\n",
    "        model = sktree.DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=123\n",
    "        )\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, \n",
    "            all_features[features], \n",
    "            jobs_df[task], \n",
    "            cv=3, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        preds = cross_val_predict(model, all_features[features], jobs_df[task], cv=3, n_jobs=-1)\n",
    "        model.fit(all_features[features], jobs_df[task])\n",
    "        with open(save_path, 'wb') as out:\n",
    "            pickle.dump((train_sizes, train_scores, test_scores, preds, model), out)\n",
    "            \n",
    "    return train_sizes, train_scores, test_scores, preds, model\n",
    "\n",
    "@interact(task=tasks, features=featurization_approaches.keys(), max_depth=(5, 20, 5), min_samples_leaf=(1, 10, 3))\n",
    "def display_report(task, features, max_depth, min_samples_leaf):\n",
    "    global _features_importances\n",
    "    train_sizes, train_scores, test_scores, preds, model = get_model(\n",
    "        task, features, max_depth, min_samples_leaf)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    learning_curve_ax = fig.add_subplot(2, 1, 1)\n",
    "    feature_importance_ax = fig.add_subplot(2, 1, 2)\n",
    "    \n",
    "    plot_learning_curve(train_sizes, train_scores, test_scores, task, ylim=(0.5, 1.0), ax=learning_curve_ax)\n",
    "    \n",
    "    feature_names = all_featurizers[features]\n",
    "    features_importances = pd.Series(data = model.feature_importances_, index=feature_names)\n",
    "    features_importances = features_importances[features_importances > 0.0]\n",
    "    try:\n",
    "        wordcloud(features_importances, title='Feature Importances', ax=feature_importance_ax)\n",
    "    except ValueError as e:\n",
    "        _features_importances = features_importances\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(skmetrics.classification_report(jobs_df[task], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Once you find the right features/parameters, consider why they worked for that problem.\n",
    "\n",
    "#### - education\n",
    "#### - experience\n",
    "#### - is_hourly\n",
    "#### - is_part_time\n",
    "#### - is_supervisor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
