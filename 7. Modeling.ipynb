{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## Modeling\n",
    "\n",
    "Now that we have explored the vocabulary of the different segments of this corpus, let's see if we can predict whether or not a document belongs in a given segment. We can use what we've learned about the vocabulary to make educated guesses about what features we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import feature_extraction as skfeatex\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn import tree as sktree\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from vocab_analysis import *\n",
    "\n",
    "import answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.read_pickle('./data/ngrams.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>experience</th>\n",
       "      <th>education</th>\n",
       "      <th>is_hourly</th>\n",
       "      <th>is_part_time</th>\n",
       "      <th>is_supervisor</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>cleaned_lemmas</th>\n",
       "      <th>sentences</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>quadrigrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE COMPANY    Employer is a midstream service...</td>\n",
       "      <td>5+</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[THE, COMPANY, Employer, is, a, midstream, ser...</td>\n",
       "      <td>[the, compani, employ, is, a, midstream, servi...</td>\n",
       "      <td>[the, company, employer, be, a, midstream, ser...</td>\n",
       "      <td>[employer, midstream, service, provider, onsho...</td>\n",
       "      <td>[[employer, midstream, service, provider, onsh...</td>\n",
       "      <td>[employer-midstream, midstream-service, servic...</td>\n",
       "      <td>[employer-midstream-service, midstream-service...</td>\n",
       "      <td>[employer-midstream-service-provider, midstrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ICR Staffing is now accepting resumes for Indu...</td>\n",
       "      <td>2-5</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[ICR, Staffing, is, now, accepting, resumes, f...</td>\n",
       "      <td>[icr, staf, is, now, accept, resum, for, indus...</td>\n",
       "      <td>[icr, staff, be, now, accept, resume, for, ind...</td>\n",
       "      <td>[icr, staff, accept, resume, industrial, maint...</td>\n",
       "      <td>[[icr, staff, accept, resume, industrial, main...</td>\n",
       "      <td>[icr-staff, staff-accept, accept-resume, resum...</td>\n",
       "      <td>[icr-staff-accept, staff-accept-resume, accept...</td>\n",
       "      <td>[icr-staff-accept-resume, staff-accept-resume-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a great position for the right person....</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[This, is, a, great, position, for, the, right...</td>\n",
       "      <td>[this, is, a, great, posit, for, the, right, p...</td>\n",
       "      <td>[this, be, a, great, position, for, the, right...</td>\n",
       "      <td>[great, right, person, healthcareseeker, estab...</td>\n",
       "      <td>[[great, right, person], [healthcareseeker, es...</td>\n",
       "      <td>[great-right, right-person, healthcareseeker-e...</td>\n",
       "      <td>[great-right-person, healthcareseeker-establis...</td>\n",
       "      <td>[healthcareseeker-establish-place-register, es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A large multi-specialty health center is expan...</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[A, large, multi, -, specialty, health, center...</td>\n",
       "      <td>[a, larg, multi, specialti, health, center, is...</td>\n",
       "      <td>[a, large, multi, specialty, health, center, b...</td>\n",
       "      <td>[large, multi, specialty, health, center, expa...</td>\n",
       "      <td>[[large, multi, specialty, health, center, exp...</td>\n",
       "      <td>[large-multi, multi-specialty, specialty-healt...</td>\n",
       "      <td>[large-multi-specialty, multi-specialty-health...</td>\n",
       "      <td>[large-multi-specialty-health, multi-specialty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB PURPOSE:    The Account Director is respon...</td>\n",
       "      <td>5+</td>\n",
       "      <td>bs-degree-needed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[JOB, PURPOSE, :, The, Account, Director, is, ...</td>\n",
       "      <td>[job, purpos, the, account, director, is, resp...</td>\n",
       "      <td>[job, purpose, the, account, director, be, res...</td>\n",
       "      <td>[purpose, account, director, responsible, mana...</td>\n",
       "      <td>[[purpose, account, director, responsible, man...</td>\n",
       "      <td>[purpose-account, account-director, director-r...</td>\n",
       "      <td>[purpose-account-director, account-director-re...</td>\n",
       "      <td>[purpose-account-director-responsible, account...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description experience  \\\n",
       "id                                                                 \n",
       "0   THE COMPANY    Employer is a midstream service...         5+   \n",
       "1   ICR Staffing is now accepting resumes for Indu...        2-5   \n",
       "2   This is a great position for the right person....       none   \n",
       "3   A large multi-specialty health center is expan...       none   \n",
       "4   JOB PURPOSE:    The Account Director is respon...         5+   \n",
       "\n",
       "           education  is_hourly  is_part_time  is_supervisor  \\\n",
       "id                                                             \n",
       "0               none      False         False           True   \n",
       "1               none      False         False          False   \n",
       "2               none      False          True          False   \n",
       "3               none      False         False          False   \n",
       "4   bs-degree-needed      False         False           True   \n",
       "\n",
       "                                               tokens  \\\n",
       "id                                                      \n",
       "0   [THE, COMPANY, Employer, is, a, midstream, ser...   \n",
       "1   [ICR, Staffing, is, now, accepting, resumes, f...   \n",
       "2   [This, is, a, great, position, for, the, right...   \n",
       "3   [A, large, multi, -, specialty, health, center...   \n",
       "4   [JOB, PURPOSE, :, The, Account, Director, is, ...   \n",
       "\n",
       "                                                stems  \\\n",
       "id                                                      \n",
       "0   [the, compani, employ, is, a, midstream, servi...   \n",
       "1   [icr, staf, is, now, accept, resum, for, indus...   \n",
       "2   [this, is, a, great, posit, for, the, right, p...   \n",
       "3   [a, larg, multi, specialti, health, center, is...   \n",
       "4   [job, purpos, the, account, director, is, resp...   \n",
       "\n",
       "                                               lemmas  \\\n",
       "id                                                      \n",
       "0   [the, company, employer, be, a, midstream, ser...   \n",
       "1   [icr, staff, be, now, accept, resume, for, ind...   \n",
       "2   [this, be, a, great, position, for, the, right...   \n",
       "3   [a, large, multi, specialty, health, center, b...   \n",
       "4   [job, purpose, the, account, director, be, res...   \n",
       "\n",
       "                                       cleaned_lemmas  \\\n",
       "id                                                      \n",
       "0   [employer, midstream, service, provider, onsho...   \n",
       "1   [icr, staff, accept, resume, industrial, maint...   \n",
       "2   [great, right, person, healthcareseeker, estab...   \n",
       "3   [large, multi, specialty, health, center, expa...   \n",
       "4   [purpose, account, director, responsible, mana...   \n",
       "\n",
       "                                            sentences  \\\n",
       "id                                                      \n",
       "0   [[employer, midstream, service, provider, onsh...   \n",
       "1   [[icr, staff, accept, resume, industrial, main...   \n",
       "2   [[great, right, person], [healthcareseeker, es...   \n",
       "3   [[large, multi, specialty, health, center, exp...   \n",
       "4   [[purpose, account, director, responsible, man...   \n",
       "\n",
       "                                              bigrams  \\\n",
       "id                                                      \n",
       "0   [employer-midstream, midstream-service, servic...   \n",
       "1   [icr-staff, staff-accept, accept-resume, resum...   \n",
       "2   [great-right, right-person, healthcareseeker-e...   \n",
       "3   [large-multi, multi-specialty, specialty-healt...   \n",
       "4   [purpose-account, account-director, director-r...   \n",
       "\n",
       "                                             trigrams  \\\n",
       "id                                                      \n",
       "0   [employer-midstream-service, midstream-service...   \n",
       "1   [icr-staff-accept, staff-accept-resume, accept...   \n",
       "2   [great-right-person, healthcareseeker-establis...   \n",
       "3   [large-multi-specialty, multi-specialty-health...   \n",
       "4   [purpose-account-director, account-director-re...   \n",
       "\n",
       "                                          quadrigrams  \n",
       "id                                                     \n",
       "0   [employer-midstream-service-provider, midstrea...  \n",
       "1   [icr-staff-accept-resume, staff-accept-resume-...  \n",
       "2   [healthcareseeker-establish-place-register, es...  \n",
       "3   [large-multi-specialty-health, multi-specialty...  \n",
       "4   [purpose-account-director-responsible, account...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `TfidfVectorizer` from scikit-learn to generate our features. It uses an analyzer to process text and then creates feature values based the $\\mbox{TF.IDF}$ of a term within a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "    Equivalent to CountVectorizer followed by TfidfTransformer.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : string {'filename', 'file', 'content'}\n",
      "        If 'filename', the sequence passed as an argument to fit is\n",
      "        expected to be a list of filenames that need reading to fetch\n",
      "        the raw content to analyze.\n",
      "\n",
      "        If 'file', the sequence items must have a 'read' method (file-like\n",
      "        object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        Otherwise the input is expected to be the sequence strings or\n",
      "        bytes items are expected to be analyzed directly.\n",
      "\n",
      "    encoding : string, 'utf-8' by default.\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'}\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode', None}\n",
      "        Remove accents during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        an direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "    analyzer : string, {'word', 'char'} or callable\n",
      "        Whether the feature should be made of word or character n-grams.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "    preprocessor : callable or None (default)\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "\n",
      "    tokenizer : callable or None (default)\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "        will be used.\n",
      "\n",
      "    stop_words : string {'english'}, list, or None (default)\n",
      "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "        list is returned. 'english' is currently the only supported string\n",
      "        value.\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    lowercase : boolean, default True\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    token_pattern : string\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "    max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float in range [0.0, 1.0] or int, default=1\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int or None, default=None\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, optional\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents.\n",
      "\n",
      "    binary : boolean, default=False\n",
      "        If True, all non-zero term counts are set to 1. This does not mean\n",
      "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
      "\n",
      "    dtype : type, optional\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    norm : 'l1', 'l2' or None, optional\n",
      "        Norm used to normalize term vectors. None for no normalization.\n",
      "\n",
      "    use_idf : boolean, default=True\n",
      "        Enable inverse-document-frequency reweighting.\n",
      "\n",
      "    smooth_idf : boolean, default=True\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : boolean, default=False\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    idf_ : array, shape = [n_features], or None\n",
      "        The learned idf vector (global term weights)\n",
      "        when ``use_idf`` is set to True, None otherwise.\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    CountVectorizer\n",
      "        Tokenize the documents and count the occurrences of token and return\n",
      "        them as a sparse matrix\n",
      "\n",
      "    TfidfTransformer\n",
      "        Apply Term Frequency Inverse Document Frequency normalization to a\n",
      "        sparse matrix of occurrence counts.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(skfeatex.text.TfidfVectorizer.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all of our functions back so we can use them as analyzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_tokenize import tokenize\n",
    "from my_lemmatize import lemmatize, english_lemmas\n",
    "from my_stopword_removal import stopword_removal\n",
    "from my_lemma_sentences import lemma_sentences\n",
    "from my_ngram_func import ngram_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `token_analyzer` will only tokenize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_analyzer(description):\n",
    "    return tokenize(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lemma_analyzer` will tokenize and lemmatize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemma_analyzer(description):\n",
    "    return lemmatize(tokenize(description), english_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_lemma_analyzer`, will tokenize, lemmatize, and then remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_lemma_analyzer(description):\n",
    "    return stopword_removal(lemmatize(tokenize(description), english_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bigram_analyzer` will split into sentences, tokenize, lemmatize, remove stop words, and then generate bigrams as our terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_analyzer(description):\n",
    "    lemmatized_sentences = lemma_sentences(description)\n",
    "    bigrams = ngram_func(2)(lemmatized_sentences)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trigram_analyzer` will split into sentences, tokenize, lemmatize, remove stop words, and then generate trigrams as our terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_analyzer(description):\n",
    "    lemmatized_sentences = lemma_sentences(description)\n",
    "    trigrams = ngram_func(3)(lemmatized_sentences)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `full_analyzer` will split into sentences, tokenize, lemmatize, remove stop words, and then generate bigrams and trigrams. It will use the cleaned lemmas, bigrams and trigrams as our terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_analyzer(description):\n",
    "    lemmatized_sentences = lemma_sentences(description)\n",
    "    unigrams = [unigram for sentence in lemmatized_sentences for unigram in sentence]\n",
    "    bigrams = ngram_func(2)(lemmatized_sentences)\n",
    "    trigrams = ngram_func(3)(lemmatized_sentences)\n",
    "    return unigrams + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tasks are\n",
    "- education: predict what level of education is needed for a job (none, associate-needed, bs-degree-needed, ms-or-phd-needed)\n",
    "- experience: predict how many years of experience is needed for a job (none, 1-2, 2-5, 5+)\n",
    "- is_hourly: predict whether a job is an hourly or not (True, False)\n",
    "- is_part_time: predict whether a job is part time or not (True, False)\n",
    "- is_supervisor: predict whether a job is a supervisory position or not (True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = ['education', 'experience', 'is_hourly', 'is_part_time', 'is_supervisor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covering how to use scikit-learn is outside the scope of this tutorial. If you want to know more about using scikit-learn, check out Sebastian Rashka's tutorial [_Learning scikit-learn -- An Introduction to Machine Learning in Python_](https://www.youtube.com/watch?v=9fOWryQq9J8). \n",
    "\n",
    "We'll be modeling with decision trees ([wiki]()) ([_Python for Data Science_ by Joe McCarthy](http://nbviewer.jupyter.org/github/gumption/Python_for_Data_Science/blob/master/Python_for_Data_Science_all.ipynb#4.-Using-Python-to-Build-and-Use-a-Simple-Decision-Tree-Classifier)). Rather than go into the details of decision trees, let's look at one.\n",
    "\n",
    "![tree](tree.png)\n",
    "\n",
    "This is a tree that was built for the is_hourly task.\n",
    "\n",
    "I've simplified working with these models by creating some widgets that let us modify some of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: finding the right features\n",
    "\n",
    "Let's try and find the right features for these tasks.\n",
    "\n",
    "I've pre-built all the models with my analyzers. Find the best performing set of features and parameters for each task.\n",
    "\n",
    "**Note**: If you want to try your own analyzers, uncomment the following cell to rename the current results folder and create your own. Know that each model can take 10 seconds to over a minute to build. It will also rename the saved features, which will also take a few minutes to regenerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! mv ./results/ ./results-pre-built/\n",
    "# ! mkdir ./results\n",
    "# ! mv ./data/all_features.pickle ./data/all_features_pre_built.pickle\n",
    "# ! mv ./data/all_featurizers.pickle ./data/all_featurizers_pre_built.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurization_approaches = OrderedDict()\n",
    "featurization_approaches['tokens'] = token_analyzer\n",
    "featurization_approaches['lemmas'] = lemma_analyzer\n",
    "featurization_approaches['clean_lemmas'] = clean_lemma_analyzer\n",
    "featurization_approaches['bigrams'] = bigram_analyzer\n",
    "featurization_approaches['trigrams'] = trigram_analyzer\n",
    "featurization_approaches['full'] = full_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features\n",
      "Loading featurizers\n"
     ]
    }
   ],
   "source": [
    "features_path = './data/all_features.pickle'\n",
    "featurizers_path = './data/all_featurizers.pickle'\n",
    "if os.path.exists(features_path):\n",
    "    print('Loading features')\n",
    "    with open(features_path) as fp:\n",
    "        all_features = pickle.load(fp)\n",
    "    print('Loading featurizers')\n",
    "    with open(featurizers_path) as fp:\n",
    "        all_featurizers = pickle.load(fp)\n",
    "else:\n",
    "    all_features = {}\n",
    "    all_featurizers = {}\n",
    "    for name, analyzer in featurization_approaches.items():\n",
    "        print(name)\n",
    "        featurizer = skfeatex.text.TfidfVectorizer(analyzer=analyzer)\n",
    "        features = featurizer.fit_transform(jobs_df['description'])\n",
    "        all_features[name] = features\n",
    "        all_featurizers[name] = featurizer.get_feature_names()\n",
    "    print('Saving features')\n",
    "    with open(features_path, 'wb') as out:\n",
    "        pickle.dump(all_features, out)\n",
    "    print('Saving featurizers')\n",
    "    with open(featurizers_path, 'wb') as out:\n",
    "        pickle.dump(all_featurizers, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af2c99033bd43a4b5fb0212181c3d80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_model(task, features, max_depth, min_samples_leaf):\n",
    "    save_path = './results/{}-{}-{}-{}.results.pickle'.format(\n",
    "        task, features, max_depth, min_samples_leaf\n",
    "    )\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path) as fp:\n",
    "            train_sizes, train_scores, test_scores, preds, model = pickle.load(fp)\n",
    "    else:\n",
    "        model = sktree.DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=123\n",
    "        )\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, \n",
    "            all_features[features], \n",
    "            jobs_df[task], \n",
    "            cv=3, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        preds = cross_val_predict(model, all_features[features], jobs_df[task], cv=3, n_jobs=-1)\n",
    "        model.fit(all_features[features], jobs_df[task])\n",
    "        with open(save_path, 'wb') as out:\n",
    "            pickle.dump((train_sizes, train_scores, test_scores, preds, model), out)\n",
    "            \n",
    "    return train_sizes, train_scores, test_scores, preds, model\n",
    "\n",
    "@interact(task=tasks, features=featurization_approaches.keys(), max_depth=(5, 20, 5), min_samples_leaf=(1, 10, 3))\n",
    "def display_report(task, features, max_depth, min_samples_leaf):\n",
    "    global _features_importances\n",
    "    train_sizes, train_scores, test_scores, preds, model = get_model(\n",
    "        task, features, max_depth, min_samples_leaf)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    learning_curve_ax = fig.add_subplot(2, 1, 1)\n",
    "    feature_importance_ax = fig.add_subplot(2, 1, 2)\n",
    "    \n",
    "    plot_learning_curve(train_sizes, train_scores, test_scores, task, ylim=(0.5, 1.0), ax=learning_curve_ax)\n",
    "    \n",
    "    feature_names = all_featurizers[features]\n",
    "    features_importances = pd.Series(data = model.feature_importances_, index=feature_names)\n",
    "    features_importances = features_importances[features_importances > 0.0]\n",
    "    try:\n",
    "        wordcloud(features_importances, title='Feature Importances', ax=feature_importance_ax)\n",
    "    except ValueError as e:\n",
    "        _features_importances = features_importances\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(skmetrics.classification_report(jobs_df[task], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Once you find the right features/parameters, consider why they worked for that problem.\n",
    "\n",
    "#### - education\n",
    "#### - experience\n",
    "#### - is_hourly\n",
    "#### - is_part_time\n",
    "#### - is_supervisor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
