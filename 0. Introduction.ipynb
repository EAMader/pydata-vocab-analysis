{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the initial analysis of a data set it is useful to gather informative summaries. This includes evaluating the available fields, by finding unique counts or by calculating summary statistics such as averages for numerical fields. These summaries help in understanding what is in the data itself, the underlying quality, and illuminate potential paths for further exploration. In structured data, this a straightforward task, but for unstructured text, different types of summaries are needed. \n",
    "\n",
    "Some useful examples for text data include a count of the number of documents in which a term occurs, and the number of times a term occurs in a document. It is still up to use figure out what a _term_ is, and how to extract it from the text.\n",
    "\n",
    "Since vocabulary terms often have variant forms, e.g. “performs” and “performing”, it is useful to pre-process and combine these forms before computing distributions. \n",
    "\n",
    "Oftentimes, we want to look at sequences of words, for example we may want to count the number of times “data science” occurs, and not just “data” and “science”. \n",
    "\n",
    "We will use the pandas Python Data Analysis Library and the Natural Language Toolkit (NLTK) to process a data set of job descriptions posted by employers in the United States, and look at the difference in vocabularies across different job segments.\n",
    "\n",
    "<img alt=\"NLTK book\" src=\"http://covers.oreilly.com/images/9780596516499/cat.gif\" align=\"left\" float=\"left\"/>  \n",
    "**(NLTK [wikipedia](https://en.wikipedia.org/wiki/Natural_Language_Toolkit))**  \n",
    "<img alt=\"Pandas\" src=\"http://pandas.pydata.org/_static/pandas_logo.png\" align=\"left\" float=\"left\"/>  \n",
    "**(pandas [wikipedia](https://en.wikipedia.org/wiki/Pandas_(software))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "#### 0. [Introduction](0. Introduction.ipynb)  \n",
    "   In this section we will cover the outline and motivation of the tutorial, as well as introduce the data set we will be using.\n",
    "#### 1. [Tokenization](1. Tokenization.ipynb)  \n",
    "   In this section we will cover tokenization. (exercise 1)\n",
    "#### 2. [TF.IDF](2. TF.IDF.ipynb)  \n",
    "   In this section we will cover how to look at vocabulary with TF.IDF.\n",
    "#### 3. [Visualization](3. Visualization.ipynb)  \n",
    "   In this section we will cover how to visualize our vocabulary with TF.IDF.\n",
    "#### 4. [Stemming and Lemmatization](4. Stemming and Lemmatization)  \n",
    "   In this section we will cover stemming and lemmatization to normalize our vocabulary. (exercise 2)\n",
    "#### 5. [Stop Words](5. Stop Words.ipynb)  \n",
    "   In this section we will cover removal of \"less useful\" words. (exercise 3)\n",
    "#### 6. [n-Grams](6. n-Grams.ipynb)  \n",
    "   In this section we will cover word sequences and introduce sentence boundary detection.\n",
    "#### 7. [Modeling](7. Modeling.ipynb)  \n",
    "   In this section we will cover using our analysis to build models for predicting our segments. (exercise 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP - fishing for meaning\n",
    "\n",
    "An analogy I like to use when talking about NLP is looking forfishing.\n",
    "\n",
    "All of the techniques we will cover in this talk can (and should) be tuned to the particular language, data, and problem that is being examined. If you are trying to find a sunfish you don't want to go buy all the equipment looking going down to the midnight zone. Similarly, if you are looking for angler fish, you will not find them by just looking at the surface.\n",
    "\n",
    "<img src=\"https://www.nwf.org/~/media/Content/Screen%20Captures/Kids/Ranger-Rick-Spreads/ocean2-JJ2014.ashx\"/>\n",
    "\n",
    "[[National Wildlife Kids - Ocean Animals](https://www.nwf.org/Kids/Ranger-Rick/Animals/Mixture-of-Species/Ocean-Animals.aspx)]\n",
    "\n",
    "We are looking to do exploratory analysis here, so we are looking at the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data we will be looking at is from a HackerRank competition Indeed had earlier this year. It contains the text of some job descriptions as well as some other attributes.\n",
    "\n",
    "**Columns**\n",
    "- id: a generated ID for each job description\n",
    "- description: the employer entered job description text\n",
    "- experience: the number of years experience required for the job, (5+, 2-5, 1-2, none)\n",
    "- education: the education level required for the job, (ms-or-phd-needed, bs-degree-needed, associate-needed, none)\n",
    "- is_hourly: true if the job is hourly, false if it is a salary job\n",
    "- is_part_time: true if the job is part-time, false if the job is full-time\n",
    "- is_supervisor: true if the job is a supervisor-position, false otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df = pd.read_csv('data/job_descriptions.tsv', sep='\\t', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring and summarizing structured data is something most of us are familiar with. If the data is categorical or boolean we can report percentages, and display a pie chart for the values. This gives us a pretty good idea of the data.\n",
    "\n",
    "For numeric columns we can look at the mean and standard deviation, as well as look at percentiles. We can bin the values and plot a histogram.\n",
    "\n",
    "### How do we explore and summarize text?  \n",
    "Let's try looking at character length of documents, and use our knowledge of how to explore numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_lengths = jobs_df['description'].apply(len)\n",
    "print(character_lengths.describe())\n",
    "plt.figure(figsize=(8, 8))\n",
    "character_lengths.plot(kind='hist', bins=100)\n",
    "plt.xlabel('#characters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you feel you have an understanding of the text now?  \n",
    "Let's look at just the alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alnum_lengths = jobs_df['description'].apply(lambda d: len([c for c in d if c.isalnum()]))\n",
    "print(alnum_lengths.describe())\n",
    "plt.figure(figsize=(8, 8))\n",
    "alnum_lengths.plot(kind='hist', bins=100)\n",
    "plt.xlabel('#alnum characters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this kind of approach does not give us a satisfying summary of the text data.\n",
    "\n",
    "In structured data, the meaning of the values is apparent from the column name (and description) and the value itself. The meaning of text though is more complicated, so we must do some processing in order to explore it.\n",
    "\n",
    "The meaning of a text is a combination of the words, their order, and the context they are in. (**Note**: this is a gross oversimplification). How do we look at these things without doing complicated natural language processing?\n",
    "\n",
    "- Words: We can get to these by splitting the text into words (tokens actually, a _word_ is more a complicated concept) in a process called tokenization. We can process these words to further explore the data.\n",
    "- Order: We don't want to parse the text in the exploratory phase, but we can look at common small sequences of words. These small sequences are called _n-grams_.\n",
    "- Context: The structured data in our data set gives us natural ways to segment the data. So we know the overall context, job descriptions in the United States, and now we can get even narrower context.\n",
    "\n",
    "The way which we use these techniques depends on the problem at hand. In this workshop we will build towards building models to predict the segments to which a job description belong. We will be using text based features to make our predictions.\n",
    "\n",
    "Here are some questions that we want to answer in our analysis.\n",
    "\n",
    "- How should we want to process the text?\n",
    "- Is the text sufficient for making predictions about which segments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few example jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jdid, row in jobs_df.sample(n=3, random_state=123).iterrows():\n",
    "    print('=' * 80)\n",
    "    print('id = ', jdid)\n",
    "    print('=' * 30)\n",
    "    print(row.description)\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT =>  [1. Tokenization](1. Tokenization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
