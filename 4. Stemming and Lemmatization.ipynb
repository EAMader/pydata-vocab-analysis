{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Analysis Workshop\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "There are more meaningful words visible in the word clouds, but it would be good to do some normalization.\n",
    "\n",
    "Let's do the following\n",
    "1. Get rid of the punctuation\n",
    "2. Treat \"Experience\" and \"experience\" as the same term\n",
    "3. Treat \"service\" and \"services\" as the same term\n",
    "\n",
    "The first two are easy, but the third is complicated. The process of combining \"service\" and \"services\", or \"requires\" and \"requirement\" is achieved by stemming or lemmatization. These process work by either removing suffixes or by looking up \"simpler\" forms respectively.\n",
    "\n",
    "There are two kinds of suffixes that we will consider removing.\n",
    "\n",
    "1. Inflectional: \"cat**s**\", \"call**ing**\", \"quick**est**\"\n",
    "2. Derivational: \"real**ize**\", \"hope**less**\", \"require**ment**\"\n",
    "\n",
    "Stemming and lemmatization (sometimes called Soft Stemming) are text marking processes that mark segments (almost always tokens) with canonical or reduced forms. In natural language processing (NLP) it is generally used to combine words with common lemmas or stems when the meaning of the inflection or derivation is not important to the problem at hand.\n",
    "\n",
    "The two techniques differ in what kind of word is returned, as well as implementation. A stem is not necessarily a \"word\", it can be part of a word like \"includ\" for \"include\" and \"including\". A lemma is the head-word of a dictionary entry. In English, stemming is usually implemented with a sequence of transformation rules, and lemmatization is implemented with a mapping from word to lemma.\n",
    "\n",
    "(Stemming [wikipedia](https://en.wikipedia.org/wiki/Stemming))  \n",
    "(Lemmatisation [wikipedia](https://en.wikipedia.org/wiki/Lemmatisation))  \n",
    "(Snowball [wikipedia](http://snowballstem.org/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import codecs\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from vocab_analysis import *\n",
    "\n",
    "import answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df = pd.read_pickle('./data/tokenized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/segments.pickle') as fp:\n",
    "    segments = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-to-lemma mapping we have is from [Laurence Anthony](http://www.laurenceanthony.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_dictionary = {}\n",
    "with codecs.open('./data/AntBNC_lemmas_ver_001.txt', encoding='UTF-8') as fp:\n",
    "    for line in fp:\n",
    "        lemma, words = line.split('->')\n",
    "        lemma = lemma.strip()\n",
    "        if '-' in lemma:\n",
    "            continue\n",
    "        words = [w.strip() for w in words.strip().split('\\t')]\n",
    "        english_dictionary[lemma] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English Lemmas\")\n",
    "print('num lemmas', len(english_dictionary))\n",
    "most_words_lemma = max(english_dictionary, key=lambda l: len(english_dictionary[l]))\n",
    "print('max num words per lemma', most_words_lemma, len(english_dictionary[most_words_lemma]))\n",
    "print(english_dictionary[most_words_lemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_lemmas = {}\n",
    "\n",
    "for lemma, forms in english_dictionary.items():\n",
    "    for form in forms:\n",
    "        if form in english_lemmas:\n",
    "            print('Multiple lemmas, \\'{}\\' and \\'{}\\' for form \\'{}\\''.format(english_lemmas[form], lemma, form))\n",
    "        english_lemmas[form] = lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: lemmatization\n",
    "\n",
    "Let's build a lemmatizer. We know have a mapping from inflected words to their lemmas, so we can build a function to convert tokens to lemmas. Remember our normalization requirements from the top.\n",
    "\n",
    "1. Get rid of the punctuation\n",
    "2. Treat \"Experience\" and \"experience\" as the same term\n",
    "3. Treat \"service\" and \"services\" as the same term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_job_description = \"\"\"\n",
    "This is a description for a generic job.\n",
    "\n",
    "The employee is expected have the following:\n",
    "    1. an A1 certifcation (recent or renewed)\n",
    "    2. experience in widget-widget interaction\n",
    "    \n",
    "She/he will be expected to be stand for 3-4 hours at a time.\n",
    "She/he won't be expected to actually create widgets.\n",
    "\n",
    "Full-time\n",
    "Salary : $50,000/yr\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(tokens, mapping):\n",
    "    \"\"\"\n",
    "    This function takes tokens and returns the lemma of the word.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list[str]\n",
    "        the list of tokens to be stemmed\n",
    "    mapping : dict[str, str]\n",
    "        the mapping from word to lemma\n",
    "    Returns\n",
    "    ----------\n",
    "    list[str]\n",
    "        for each token, either the lemma or the token\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Implement the lemmatizer')\n",
    "\n",
    "# lemmatize = answers.lemmatize # uncomment this, and comment the above function to skip this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    This function will stem all the tokens in a given list with the Snowball English stemmer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list[str]\n",
    "        the list of tokens to be stemmed\n",
    "    Returns\n",
    "    ----------\n",
    "    list[str]\n",
    "        the list of stems with the non-alphabetic words removed\n",
    "    \"\"\"\n",
    "    return [stemmer.stem(t) for t in tokens if t.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jobs_df['stems'] = jobs_df['tokens'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jobs_df['lemmas'] = jobs_df['tokens'].apply(lambda tokens: lemmatize(tokens, english_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the effects of stemming on $\\mbox{TF.IDF}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze(jobs_df, 'stems', segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(jobs_df, 'lemmas', segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save off our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_fun(stem, imports=['nltk'], stemmer=stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_fun(lemmatize, english_lemmas=english_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df.to_pickle('./data/lemmatized.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see more interesting words than before, but \"and\", \"to\", \"the\" still seem to dominate. We need some way of removing these troublesome words.\n",
    "\n",
    "### NEXT => [5. Stop Words](5. Stop Words.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
